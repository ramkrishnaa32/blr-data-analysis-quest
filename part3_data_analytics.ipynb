{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data Analytics with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T05:32:49.827748Z",
     "start_time": "2025-09-04T05:32:49.784219Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import boto3\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T05:32:54.674838Z",
     "start_time": "2025-09-04T05:32:54.617080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/04 11:15:06 WARN Utils: Your hostname, Rams-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.1.3 instead (on interface en0)\n",
      "25/09/04 11:15:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/kramkrishnaachary/Desktop/Rearc/quest/.venv/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/09/04 11:15:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RearcDataAnalytics\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "BUCKET_NAME = 'rearc-data-quest-us-east-01'\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T05:33:07.238453Z",
     "start_time": "2025-09-04T05:32:59.218950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLS data: 37182 rows\n",
      "+-----------+----+------+-----+--------------+\n",
      "|series_id  |year|period|value|footnote_codes|\n",
      "+-----------+----+------+-----+--------------+\n",
      "|PRS30006011|1995|Q01   |2.6  |null          |\n",
      "|PRS30006011|1995|Q02   |2.1  |null          |\n",
      "|PRS30006011|1995|Q03   |0.9  |null          |\n",
      "|PRS30006011|1995|Q04   |0.1  |null          |\n",
      "|PRS30006011|1995|Q05   |1.4  |null          |\n",
      "+-----------+----+------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load BLS data\n",
    "obj = s3_client.get_object(Bucket=BUCKET_NAME, Key='bls-data/pub/time.series/pr/pr.data.0.Current')\n",
    "csv_content = obj['Body'].read().decode('utf-8')\n",
    "\n",
    "# Create DataFrame from CSV content\n",
    "from pyspark.sql import Row\n",
    "import csv\n",
    "\n",
    "lines = csv_content.strip().split('\\n')\n",
    "reader = csv.DictReader(StringIO(csv_content), delimiter='\\t')\n",
    "rows = [Row(**{k.strip(): v.strip() if v else None for k, v in row.items()}) for row in reader]\n",
    "\n",
    "bls_df = spark.createDataFrame(rows)\n",
    "bls_df = bls_df.withColumn(\"year\", col(\"year\").cast(IntegerType())) \\\n",
    "             .withColumn(\"value\", col(\"value\").cast(DoubleType()))\n",
    "\n",
    "print(f\"BLS data: {bls_df.count()} rows\")\n",
    "\n",
    "print(bls_df.show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T05:33:26.060082Z",
     "start_time": "2025-09-04T05:33:24.516828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population data: 10 rows\n",
      "+---------+-------------+----+----------+\n",
      "|Nation ID|Nation       |Year|Population|\n",
      "+---------+-------------+----+----------+\n",
      "|01000US  |United States|2013|316128839 |\n",
      "|01000US  |United States|2014|318857056 |\n",
      "|01000US  |United States|2015|321418821 |\n",
      "|01000US  |United States|2016|323127515 |\n",
      "|01000US  |United States|2017|325719178 |\n",
      "|01000US  |United States|2018|327167439 |\n",
      "|01000US  |United States|2019|328239523 |\n",
      "|01000US  |United States|2021|331893745 |\n",
      "|01000US  |United States|2022|333287562 |\n",
      "|01000US  |United States|2023|334914896 |\n",
      "+---------+-------------+----+----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load population data\n",
    "obj = s3_client.get_object(Bucket=BUCKET_NAME, Key='api-data/population_data.json')\n",
    "json_data = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "\n",
    "# Extract data array\n",
    "if 'data' in json_data and 'data' in json_data['data']:\n",
    "    pop_data = json_data['data']['data']\n",
    "else:\n",
    "    pop_data = json_data.get('data', json_data)\n",
    "\n",
    "pop_df = spark.createDataFrame([Row(**row) for row in pop_data])\n",
    "pop_df = pop_df.withColumn(\"Year\", col(\"Year\").cast(IntegerType())) \\\n",
    "             .withColumn(\"Population\", col(\"Population\").cast(LongType()))\n",
    "\n",
    "print(f\"Population data: {pop_df.count()} rows\")\n",
    "print(pop_df.orderBy('Year').show(truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: Population Statistics (2013-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T05:33:33.589735Z",
     "start_time": "2025-09-04T05:33:33.425021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Population: 322,069,808\n",
      "Std Deviation: 4,158,441\n"
     ]
    }
   ],
   "source": [
    "# Filter and calculate population stats\n",
    "pop_filtered = pop_df.filter((col(\"Year\") >= 2013) & (col(\"Year\") <= 2018))\n",
    "stats = pop_filtered.agg(\n",
    "    mean(\"Population\").alias(\"mean_pop\"),\n",
    "    stddev(\"Population\").alias(\"std_pop\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Mean Population: {stats.mean_pop:,.0f}\")\n",
    "print(f\"Std Deviation: {stats.std_pop:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: Best Year per Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T05:33:41.126802Z",
     "start_time": "2025-09-04T05:33:39.654222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total series analyzed: 282\n",
      "+-----------+----+------------------+\n",
      "|series_id  |year|value             |\n",
      "+-----------+----+------------------+\n",
      "|PRS30006011|2022|20.5              |\n",
      "|PRS30006012|2022|17.1              |\n",
      "|PRS30006013|1998|705.895           |\n",
      "|PRS30006021|2010|17.7              |\n",
      "|PRS30006022|2010|12.399999999999999|\n",
      "|PRS30006023|2014|503.21600000000007|\n",
      "|PRS30006031|2022|20.5              |\n",
      "|PRS30006032|2021|17.1              |\n",
      "|PRS30006033|1998|702.672           |\n",
      "|PRS30006061|2022|37.0              |\n",
      "+-----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter quarterly data and sum by series/year\n",
    "quarterly_df = bls_df.filter(col(\"period\").startswith(\"Q\"))\n",
    "\n",
    "yearly_sums = quarterly_df.groupBy(\"series_id\", \"year\") \\\n",
    "    .agg(sum(\"value\").alias(\"total_value\"))\n",
    "\n",
    "# Best year for each series\n",
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy(\"series_id\").orderBy(desc(\"total_value\"))\n",
    "\n",
    "best_years = yearly_sums.withColumn(\"rank\", row_number().over(window)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"series_id\", \"year\", col(\"total_value\").alias(\"value\")) \\\n",
    "    .orderBy(\"series_id\")\n",
    "\n",
    "print(f\"Total series analyzed: {best_years.count()}\")\n",
    "best_years.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3: Combined Report"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T05:52:20.960155Z",
     "start_time": "2025-09-04T05:52:20.385581Z"
    }
   },
   "source": [
    "# Filter specific series and join with population data\n",
    "filtered_bls = bls_df.filter(\n",
    "    (col(\"series_id\") == \"PRS30006032\") & \n",
    "    (col(\"period\") == \"Q01\")\n",
    ")\n",
    "\n",
    "combined = filtered_bls.join(\n",
    "    pop_df.select(col(\"year\").alias(\"pop_year\"), \"Population\"),\n",
    "    filtered_bls.year == col(\"pop_year\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    \"series_id\", \"year\", \"period\", \"value\", \"Population\"\n",
    ").orderBy(\"year\")\n",
    "\n",
    "total_records = combined.count()\n",
    "with_population = combined.filter(col(\"Population\").isNotNull()).count()\n",
    "\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Records with population: {with_population}\")\n",
    "combined.show(truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 31\n",
      "Records with population: 10\n",
      "+-----------+----+------+-----+----------+\n",
      "|series_id  |year|period|value|Population|\n",
      "+-----------+----+------+-----+----------+\n",
      "|PRS30006032|1995|Q01   |0.0  |null      |\n",
      "|PRS30006032|1996|Q01   |-4.2 |null      |\n",
      "|PRS30006032|1997|Q01   |2.8  |null      |\n",
      "|PRS30006032|1998|Q01   |0.9  |null      |\n",
      "|PRS30006032|1999|Q01   |-4.1 |null      |\n",
      "|PRS30006032|2000|Q01   |0.5  |null      |\n",
      "|PRS30006032|2001|Q01   |-6.3 |null      |\n",
      "|PRS30006032|2002|Q01   |-6.6 |null      |\n",
      "|PRS30006032|2003|Q01   |-5.7 |null      |\n",
      "|PRS30006032|2004|Q01   |2.0  |null      |\n",
      "|PRS30006032|2005|Q01   |-0.5 |null      |\n",
      "|PRS30006032|2006|Q01   |1.8  |null      |\n",
      "|PRS30006032|2007|Q01   |-0.8 |null      |\n",
      "|PRS30006032|2008|Q01   |-3.5 |null      |\n",
      "|PRS30006032|2009|Q01   |-21.0|null      |\n",
      "|PRS30006032|2010|Q01   |3.2  |null      |\n",
      "|PRS30006032|2011|Q01   |1.5  |null      |\n",
      "|PRS30006032|2012|Q01   |2.5  |null      |\n",
      "|PRS30006032|2013|Q01   |0.5  |316128839 |\n",
      "|PRS30006032|2014|Q01   |-0.1 |318857056 |\n",
      "+-----------+----+------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
